---
kind: Deployment
apiVersion: apps/v1
metadata:
annotations:
  deployment.kubernetes.io/revision: "1"
generation: 1
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: alertmanager
  heritage: Tiller
  release: prometheus
name: prometheus-alertmanager
namespace: monitoring
spec:
progressDeadlineSeconds: 2147483647
replicas: 1
revisionHistoryLimit: 2147483647
selector:
  matchLabels:
    app: prometheus
    component: alertmanager
    release: prometheus
strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
  type: RollingUpdate
template:
  metadata:
    creationTimestamp: null
    labels:
      app: prometheus
      chart: prometheus-9.1.0
      component: alertmanager
      heritage: Tiller
      release: prometheus
  spec:
    containers:
    - args:
      - --config.file=/etc/config/alertmanager.yml
      - --storage.path=/data
      - --cluster.advertise-address=$(POD_IP):6783
      - --web.external-url=/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: prom/alertmanager:v0.18.0
      imagePullPolicy: IfNotPresent
      name: prometheus-alertmanager
      ports:
      - containerPort: 9093
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /#/status
          port: 9093
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
    - args:
      - --volume-dir=/etc/config
      - --webhook-url=http://127.0.0.1:9093/-/reload
      image: jimmidyson/configmap-reload:v0.2.2
      imagePullPolicy: IfNotPresent
      name: prometheus-alertmanager-configmap-reload
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
    dnsPolicy: ClusterFirst
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-alertmanager
    serviceAccountName: prometheus-alertmanager
    terminationGracePeriodSeconds: 30
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-alertmanager
      name: config-volume
    - emptyDir: {}
      name: storage-volume
---

kind: Deployment
apiVersion: apps/v1
metadata:
annotations:
  deployment.kubernetes.io/revision: "1"
generation: 1
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: kube-state-metrics
  heritage: Tiller
  release: prometheus
name: prometheus-kube-state-metrics
namespace: monitoring
spec:
progressDeadlineSeconds: 2147483647
replicas: 1
revisionHistoryLimit: 2147483647
selector:
  matchLabels:
    app: prometheus
    component: kube-state-metrics
    release: prometheus
strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
  type: RollingUpdate
template:
  metadata:
    creationTimestamp: null
    labels:
      app: prometheus
      chart: prometheus-9.1.0
      component: kube-state-metrics
      heritage: Tiller
      release: prometheus
  spec:
    containers:
    - image: quay.io/coreos/kube-state-metrics:v1.6.0
      imagePullPolicy: IfNotPresent
      name: prometheus-kube-state-metrics
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
---
kind: Deployment
apiVersion: apps/v1
metadata:
annotations:
  deployment.kubernetes.io/revision: "1"
generation: 1
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: pushgateway
  heritage: Tiller
  release: prometheus
name: prometheus-pushgateway
namespace: monitoring
spec:
progressDeadlineSeconds: 2147483647
replicas: 1
revisionHistoryLimit: 2147483647
selector:
  matchLabels:
    app: prometheus
    component: pushgateway
    release: prometheus
strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
  type: RollingUpdate
template:
  metadata:
    creationTimestamp: null
    labels:
      app: prometheus
      chart: prometheus-9.1.0
      component: pushgateway
      heritage: Tiller
      release: prometheus
  spec:
    containers:
    - image: prom/pushgateway:v0.8.0
      imagePullPolicy: IfNotPresent
      name: prometheus-pushgateway
      ports:
      - containerPort: 9091
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /#/status
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-pushgateway
    serviceAccountName: prometheus-pushgateway
    terminationGracePeriodSeconds: 30

---
kind: Deployment
apiVersion: apps/v1
metadata:
annotations:
  deployment.kubernetes.io/revision: "1"
generation: 1
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: server
  heritage: Tiller
  release: prometheus
name: prometheus-server
namespace: monitoring
spec:
progressDeadlineSeconds: 2147483647
replicas: 1
revisionHistoryLimit: 2147483647
selector:
  matchLabels:
    app: prometheus
    component: server
    release: prometheus
strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
  type: RollingUpdate
template:
  metadata:
    creationTimestamp: null
    labels:
      app: prometheus
      chart: prometheus-9.1.0
      component: server
      heritage: Tiller
      release: prometheus
  spec:
    containers:
    - args:
      - --volume-dir=/etc/config
      - --webhook-url=http://127.0.0.1:9090/-/reload
      image: jimmidyson/configmap-reload:v0.2.2
      imagePullPolicy: IfNotPresent
      name: prometheus-server-configmap-reload
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: prom/prometheus:v2.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
    dnsPolicy: ClusterFirst
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-server
    serviceAccountName: prometheus-server
    terminationGracePeriodSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-server
      name: config-volume
    - emptyDir: {}
      name: storage-volume

---

kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
  generation: 3
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    version: v0.3.1
  name: metrics-server
  namespace: kube-system
spec:
  progressDeadlineSeconds: 2147483647
  replicas: 1
  revisionHistoryLimit: 2147483647
  selector:
    matchLabels:
      k8s-app: metrics-server
      version: v0.3.1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
        seccomp.security.alpha.kubernetes.io/pod: docker/default
      creationTimestamp: null
      labels:
        k8s-app: metrics-server
        version: v0.3.1
      name: metrics-server
    spec:
      containers:
      - command:
        - /metrics-server
        - --metric-resolution=30s
        - --kubelet-preferred-address-types=InternalIP
        - --kubelet-insecure-tls
        image: k8s.gcr.io/metrics-server-amd64:v0.3.1
        imagePullPolicy: IfNotPresent
        name: metrics-server
        ports:
        - containerPort: 443
          name: https
          protocol: TCP
        resources:
          limits:
            cpu: 420m
            memory: 260Mi
          requests:
            cpu: 420m
            memory: 260Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - command:
        - /pod_nanny
        - --config-dir=/etc/config
        - --cpu=300m
        - --extra-cpu=20m
        - --memory=200Mi
        - --extra-memory=10Mi
        - --threshold=5
        - --deployment=metrics-server
        - --container=metrics-server
        - --poll-period=300000
        - --estimator=exponential
        - --minClusterSize=2
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: k8s.gcr.io/addon-resizer:1.8.4
        imagePullPolicy: IfNotPresent
        name: metrics-server-nanny
        resources:
          limits:
            cpu: 420m
            memory: 256Mi
          requests:
            cpu: 200m
            memory: 128Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/config
          name: metrics-server-config-volume
      dnsPolicy: ClusterFirst
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: metrics-server
      serviceAccountName: metrics-server
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - configMap:
          defaultMode: 420
          name: metrics-server-config
        name: metrics-server-config-volume

---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"dashboard-metrics-scraper"},"name":"dashboard-metrics-scraper","namespace":"kubernetes-dashboard"},"spec":{"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"k8s-app":"dashboard-metrics-scraper"}},"template":{"metadata":{"labels":{"k8s-app":"dashboard-metrics-scraper"}},"spec":{"containers":[{"image":"kubernetesui/metrics-scraper:v1.0.1","livenessProbe":{"httpGet":{"path":"/","port":8000,"scheme":"HTTP"},"initialDelaySeconds":30,"timeoutSeconds":30},"name":"dashboard-metrics-scraper","ports":[{"containerPort":8000,"protocol":"TCP"}],"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"serviceAccountName":"kubernetes-dashboard","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"emptyDir":{},"name":"tmp-volume"}]}}}}
  generation: 1
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: dashboard-metrics-scraper
    spec:
      containers:
      - image: kubernetesui/metrics-scraper:v1.0.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        name: dashboard-metrics-scraper
        ports:
        - containerPort: 8000
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /tmp
          name: tmp-volume
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kubernetes-dashboard
      serviceAccountName: kubernetes-dashboard
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - emptyDir: {}
        name: tmp-volume

---


kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kubernetes-dashboard"},"spec":{"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"k8s-app":"kubernetes-dashboard"}},"template":{"metadata":{"labels":{"k8s-app":"kubernetes-dashboard"}},"spec":{"containers":[{"args":["--auto-generate-certificates","--namespace=kubernetes-dashboard"],"image":"kubernetesui/dashboard:v2.0.0-beta4","imagePullPolicy":"Always","livenessProbe":{"httpGet":{"path":"/","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":30,"timeoutSeconds":30},"name":"kubernetes-dashboard","ports":[{"containerPort":8443,"protocol":"TCP"}],"volumeMounts":[{"mountPath":"/certs","name":"kubernetes-dashboard-certs"},{"mountPath":"/tmp","name":"tmp-volume"}]}],"serviceAccountName":"kubernetes-dashboard","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"name":"kubernetes-dashboard-certs","secret":{"secretName":"kubernetes-dashboard-certs"}},{"emptyDir":{},"name":"tmp-volume"}]}}}}
  generation: 1
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - args:
        - --auto-generate-certificates
        - --namespace=kubernetes-dashboard
        image: kubernetesui/dashboard:v2.0.0-beta4
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        name: kubernetes-dashboard
        ports:
        - containerPort: 8443
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /certs
          name: kubernetes-dashboard-certs
        - mountPath: /tmp
          name: tmp-volume
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kubernetes-dashboard
      serviceAccountName: kubernetes-dashboard
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          defaultMode: 420
          secretName: kubernetes-dashboard-certs
      - emptyDir: {}
        name: tmp-volume

---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  annotations:
    deprecated.daemonset.template.generation: "1"
  generation: 1
  labels:
    app: prometheus
    chart: prometheus-9.1.0
    component: node-exporter
    heritage: Tiller
    release: prometheus
  name: prometheus-node-exporter
  namespace: monitoring
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: prometheus
      component: node-exporter
      release: prometheus
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: prometheus
        chart: prometheus-9.1.0
        component: node-exporter
        heritage: Tiller
        release: prometheus
    spec:
      containers:
      - args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        image: prom/node-exporter:v0.18.0
        imagePullPolicy: IfNotPresent
        name: prometheus-node-exporter
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: metrics
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /host/proc
          name: proc
          readOnly: true
        - mountPath: /host/sys
          name: sys
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      hostPID: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: prometheus-node-exporter
      serviceAccountName: prometheus-node-exporter
      terminationGracePeriodSeconds: 30
      volumes:
      - hostPath:
          path: /proc
          type: ""
        name: proc
      - hostPath:
          path: /sys
          type: ""
        name: sys
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
---

kind: DaemonSet
piVersion: apps/v1
metadata:
  annotations:
    deprecated.daemonset.template.generation: "28"
    meta.helm.sh/release-name: fluentd-elasticsearch
    meta.helm.sh/release-namespace: elk
  generation: 28
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    app.kubernetes.io/instance: fluentd-elasticsearch
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: fluentd-elasticsearch
    app.kubernetes.io/version: 3.0.4
    helm.sh/chart: fluentd-elasticsearch-10.0.0
  name: fluentd-elasticsearch
  namespace: elk
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: fluentd-elasticsearch
      app.kubernetes.io/name: fluentd-elasticsearch
  template:
    metadata:
      annotations:
        checksum/config: a4c10cb2881d4efffcb5d75f00f5beb8e405c6529c18de724bc137eef4d78299
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: fluentd-elasticsearch
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: fluentd-elasticsearch
        app.kubernetes.io/version: 3.0.4
        helm.sh/chart: fluentd-elasticsearch-10.0.0
    spec:
      containers:
      - env:
        - name: OUTPUT_BUFFER_RETRY_MAX_TIMES
          value: "5"
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        - name: OUTPUT_HOSTS
          value: 172.30.136.115:9200
        - name: OUTPUT_PATH
        - name: OUTPUT_USER
          value: bfuser
        - name: OUTPUT_PASSWORD
          value: "123456"
        - name: LOGSTASH_FORMAT
          value: "true"
        - name: LOGSTASH_DATEFORMAT
          value: '%Y.%m.%d'
        - name: LOGSTASH_PREFIX
          value: logstash
        - name: LOGSTASH_PREFIX_SEPARATOR
          value: '-'
        - name: INDEX_NAME
          value: fluentd
        - name: OUTPUT_SCHEME
          value: http
        - name: OUTPUT_TYPE
          value: elasticsearch
        - name: OUTPUT_SSL_VERIFY
          value: "false"
        - name: OUTPUT_SSL_VERSION
          value: TLSv1_2
        - name: OUTPUT_TYPE_NAME
          value: _doc
        - name: OUTPUT_BUFFER_CHUNK_LIMIT
          value: 8M
        - name: OUTPUT_BUFFER_QUEUE_LIMIT
          value: "64"
        - name: OUTPUT_BUFFER_TYPE
          value: file
        - name: OUTPUT_BUFFER_PATH
          value: /var/log/fluentd-buffers/kubernetes.system.buffer
        - name: OUTPUT_BUFFER_FLUSH_MODE
          value: interval
        - name: OUTPUT_BUFFER_RETRY_TYPE
          value: exponential_backoff
        - name: OUTPUT_BUFFER_FLUSH_THREAD_TYPE
          value: "16"
        - name: OUTPUT_BUFFER_FLUSH_INTERVAL
          value: 15s
        - name: OUTPUT_BUFFER_RETRY_FOREVER
          value: "false"
        - name: OUTPUT_BUFFER_RETRY_MAX_INTERVAL
          value: "30"
        - name: OUTPUT_BUFFER_OVERFLOW_ACTION
          value: drop_oldest_chunk
        - name: OUTPUT_LOG_LEVEL
          value: info
        - name: OUTPUT_INCLUDE_TAG_KEY
          value: "true"
        - name: OUTPUT_RECONNECT_ON_ERROR
          value: "true"
        - name: OUTPUT_RELOAD_ON_FAILURE
          value: "false"
        - name: OUTPUT_RELOAD_CONNECTIONS
          value: "false"
        - name: OUTPUT_REQUEST_TIMEOUT
          value: 30s
        - name: OUTPUT_SUPPRESS_TYPE_NAME
          value: "true"
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: quay.io/fluentd_elasticsearch/fluentd:v3.3.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
                echo "Expected directory /var/log/fluentd-buffers does not exist. This is likely a configuration issue.";
                exit 1;
              fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [ -n "$(find /var/log/fluentd-buffers -mindepth 1 -type d ! -newer /tmp/marker-stuck -print -quit)" ]; then
                echo "Elasticsearch buffers found stuck longer than $STUCK_THRESHOLD_SECONDS seconds. Clearing buffers."
                rm -rf /var/log/fluentd-buffers;
                exit 1;
              fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [ -n "$(find /var/log/fluentd-buffers -mindepth 1 -type d ! -newer /tmp/marker-liveness -print -quit)" ]; then
                echo "Elasticsearch buffers found stuck longer than $LIVENESS_THRESHOLD_SECONDS seconds."
                exit 1;
              fi;
          failureThreshold: 3
          initialDelaySeconds: 600
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 1
        name: fluentd-elasticsearch
        resources:
          limits:
            cpu: "4"
            memory: 4Gi
          requests:
            cpu: 200m
            memory: 512Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/log
          name: varlog
        - mountPath: /var/lib/docker/containers
          name: varlibdockercontainers
          readOnly: true
        - mountPath: /usr/lib64
          name: libsystemddir
          readOnly: true
        - mountPath: /etc/fluent/config.d
          name: config-volume
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: fluentd-elasticsearch
      serviceAccountName: fluentd-elasticsearch
      terminationGracePeriodSeconds: 30
      volumes:
      - hostPath:
          path: /var/log
          type: ""
        name: varlog
      - hostPath:
          path: /var/lib/docker/containers
          type: ""
        name: varlibdockercontainers
      - hostPath:
          path: /usr/lib64
          type: ""
        name: libsystemddir
      - configMap:
          defaultMode: 420
          name: fluentd-elasticsearch
        name: config-volume
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate

---
kind: Service
apiVersion: v1
metadata:
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: alertmanager
  heritage: Tiller
  release: prometheus
name: prometheus-alertmanager
namespace: monitoring
spec:
ports:
- name: http
  port: 80
  protocol: TCP
  targetPort: 9093
selector:
  app: prometheus
  component: alertmanager
  release: prometheus
sessionAffinity: None
type: ClusterIP

---

kind: Service
apiVersion: v1
metadata:
annotations:
  prometheus.io/scrape: "true"
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: kube-state-metrics
  heritage: Tiller
  release: prometheus
name: prometheus-kube-state-metrics
namespace: monitoring
spec:
clusterIP: None
ports:
- name: http
  port: 80
  protocol: TCP
  targetPort: 8080
selector:
  app: prometheus
  component: kube-state-metrics
  release: prometheus
sessionAffinity: None
type: ClusterIP

---

kind: Service
apiVersion: v1
metadata:
annotations:
  prometheus.io/scrape: "true"
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: node-exporter
  heritage: Tiller
  release: prometheus
name: prometheus-node-exporter
namespace: monitoring
spec:
clusterIP: None
ports:
- name: metrics
  port: 9100
  protocol: TCP
  targetPort: 9100
selector:
  app: prometheus
  component: node-exporter
  release: prometheus
sessionAffinity: None
type: ClusterIP
---
kind: Service
apiVersion: v1
metadata:
annotations:
  prometheus.io/probe: pushgateway
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: pushgateway
  heritage: Tiller
  release: prometheus
name: prometheus-pushgateway
namespace: monitoring
spec:
ports:
- name: http
  port: 9091
  protocol: TCP
  targetPort: 9091
selector:
  app: prometheus
  component: pushgateway
  release: prometheus
sessionAffinity: None
type: ClusterIP
---

kind: Service
apiVersion: v1
metadata:
labels:
  app: prometheus
  chart: prometheus-9.1.0
  component: server
  heritage: Tiller
  release: prometheus
name: prometheus-server
namespace: monitoring
spec:
externalTrafficPolicy: Cluster
ports:
- name: http
  nodePort: 31439
  port: 80
  protocol: TCP
  targetPort: 9090
selector:
  app: prometheus
  component: server
  release: prometheus
sessionAffinity: None
type: NodePort